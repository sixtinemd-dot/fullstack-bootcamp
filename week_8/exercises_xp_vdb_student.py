# -*- coding: utf-8 -*-
"""Exercises_XP_VDB_Student.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JuNt57_IB0v9ppm8_x-pwXsdZhIA4wO7

# Exercises XP: Vector Databases and RAG
Use this guided notebook and fill each TODO before running cells.

## What you'll learn
- Vector search strategies (KNN, ANN) and evaluation.
- Vector database utility (similarity search, RAG).
- Differences between vector DBs, libraries, and plugins.
- Best practices for vector store usage and performance.
- How LMs use context; embedding generation and storage.
- Querying vector stores and applying LMs for QA with retrieved context.

## What you'll build
A functional RAG pipeline with FAISS and ChromaDB, plus QA over retrieved context using a Hugging Face model.

## 0. Setup
Run the install cell once. If your platform needs system deps (e.g., libomp for FAISS), follow instructions in comments.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip uninstall -y pydantic-core pydantic
# %pip install -U "pydantic<2"
# %pip install -U "faiss-cpu>=1.8.0" "chromadb==0.3.21"
# %pip install -U "numpy<2" sentence-transformers transformers

import os
import json
from pathlib import Path
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer, InputExample
import chromadb
from chromadb.config import Settings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from IPython.display import display
os.makedirs('cache', exist_ok=True)

"""## ðŸŒŸ Exercise 1 Â· Data loading and preparation"""

data_path = 'labeled_newscatcher_dataset.csv'
pdf = pd.read_csv(data_path, sep=';')

if 'id' not in pdf.columns:
    pdf['id'] = range(len(pdf))

display(pdf.head())

pdf_subset = pdf.iloc[:1000].copy()
pdf_subset[['id', 'title']].head()

"""## ðŸŒŸ Exercise 2 Â· Vectorization with Sentence Transformers"""

from sentence_transformers import InputExample, SentenceTransformer

# Helper function is already defined
def example_create_fn(idx: int, text: str) -> InputExample:
    return InputExample(guid=str(idx), texts=[text], label=0.0)

# âœ… Create training examples from the subset
faiss_train_examples = [
    example_create_fn(row.id, row.title)
    for _, row in pdf_subset.iterrows()
]

# Preview first 2 examples
faiss_train_examples[:2]

model = SentenceTransformer('all-MiniLM-L6-v2')
titles_list = pdf_subset['title'].tolist()
faiss_title_embedding = model.encode(titles_list, convert_to_numpy=True, show_progress_bar=True)
len(faiss_title_embedding), len(faiss_title_embedding[0])

"""## ðŸŒŸ Exercise 3 Â· FAISS indexing and search"""

pdf_to_index = pdf_subset
id_index = pdf_to_index['id'].to_numpy().astype(np.int64)
content_encoded_normalized = faiss_title_embedding.astype('float32')
faiss.normalize_L2(content_encoded_normalized)
index_content = faiss.IndexIDMap(faiss.IndexFlatIP(content_encoded_normalized.shape[1]))
index_content.add_with_ids(content_encoded_normalized, id_index)
index_content.ntotal

def search_content(query: str, pdf_to_index: pd.DataFrame, k: int = 3):
    # Encode the query using the same sentence transformer model
    query_vector = model.encode([query], convert_to_numpy=True).astype('float32')

    # Normalize the query vector
    faiss.normalize_L2(query_vector)

    # Search the FAISS index
    sims, ids = index_content.search(query_vector, k)

    # Retrieve matching rows
    results = pdf_to_index[pdf_to_index['id'].isin(ids[0])].copy()

    # Add similarity scores
    results['similarities'] = sims[0][:len(results)]

    # Sort by similarity descending
    results = results.sort_values(by='similarities', ascending=False)

    return results

# Test the search function
display(search_content('animal', pdf_to_index, k=5))

"""## ðŸŒŸ Exercise 4 Â· ChromaDB collection and querying"""

import chromadb
from chromadb.config import Settings
import json

# Initialize ChromaDB client
chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))

collection_name = 'my_news'

# Delete existing collection if it exists
if any(c.name == collection_name for c in chroma_client.list_collections()):
    chroma_client.delete_collection(name=collection_name)

# Create a new collection
collection = chroma_client.create_collection(name=collection_name)

# Add first 100 titles with metadata and unique IDs
collection.add(
    documents=pdf_subset["title"][:100].tolist(),
    metadatas=[{"topic": t} for t in pdf_subset["topic"][:100].tolist()],
    ids=[str(i) for i in pdf_subset["id"][:100].tolist()]
)

# Query the collection for documents related to 'space'
results = collection.query(
    query_texts=["space"],
    n_results=10
)

# Print results neatly
print(json.dumps(results, indent=2))

"""## ðŸŒŸ Exercise 5 Â· Question answering with a Hugging Face model"""

from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

model_id = 'google/flan-t5-small'  # lightweight QA model

# âœ… Create the text2text-generation pipeline
pipe = pipeline(
    "text2text-generation",
    model=model_id,
    tokenizer=model_id,
    device_map="auto"  # uses GPU if available
)

# Define your question and context
question = "What's the latest news on space development?"

# Use top 3 retrieved documents from ChromaDB as context
context_docs = results['documents'][0][:3]
context = ' '.join(context_docs)

# Build the prompt
prompt = f"Answer the question using only the context.\nContext: {context}\nQuestion: {question}\nAnswer:\n"

# Generate the answer
response = pipe(prompt)[0]['generated_text']

print(response)